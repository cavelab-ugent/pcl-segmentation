dataset:
  name: TropicalLeafWood
  dataset_path: /mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/preprocessed_open3d
  train_dir: train
  val_dir: val
  test_dir: test
  cache_dir: ./logs/cache_small3d/
  class_weights: [34512040, 9127323]
  ignored_label_inds: []
  num_points: 65536
  test_result_folder: ./test_pred_randlanet
  use_cache: true
  # val_files:
  # - bildstein_station1_xyz_intensity_rgb
  # - domfountain_station1_xyz_intensity_rgb
  # steps_per_epoch_train: 500
  # steps_per_epoch_valid: 10
model:
  name: RandLANet
  batcher: DefaultBatcher
  # device: 'cpu'
  # ckpt_path: logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00100.pth # continue from this checkpoint (=saved weights)
  # is_resume: True
  num_neighbors: 16
  num_layers: 5
  num_points: 65536 # =2^16. A random sample of num_points will be taken from each point cloud as input for the model
  num_classes: 2
  class_weights: [34512040, 9127323]
  ignored_label_inds: []
  sub_sampling_ratio: [4, 4, 4, 4, 2] # 
  in_channels: 3
  dim_features: 3
  dim_output: [16, 64, 128, 256, 512]
  grid_size: 0.02 # Point clouds will be preprocessed with this grid size
  augment: # TODO: rebase z-values. Check influence of different augmentations.
    recenter:
      dim: [0, 1, 2]
    # normalize:
    #   feat:
    #     method: linear
    #     bias: 0
    #     scale: 255
    rotate:
      method: vertical
    scale:
      min_s: 0.9
      max_s: 1.1
    noise:
      noise_std: 0.001
pipeline:
  name: SemanticSegmentation
  device: 'cuda:0'
  split: train
  optimizer:
    lr: 0.001
  batch_size: 1
  main_log_dir: ./logs
  max_epoch: 100
  save_ckpt_freq: 5
  scheduler_gamma: 0.9886
  test_batch_size: 1
  train_sum_dir: train_log
  val_batch_size: 1
  summary:
    record_for: []
    max_pts:
    use_reference: false
    max_outputs: 1
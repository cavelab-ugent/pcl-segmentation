dataset:
  name: TropicalLeafWood
  dataset_path: /project/data
  train_dir: train
  val_dir: val
  test_dir: test
  cache_dir: /project/logs/cache
  class_weights: [34512040, 9127323]
  ignored_label_inds: []
  num_points: 65536
  # test_result_folder: ./test
  use_cache: true
  # val_files:
  # - bildstein_station1_xyz_intensity_rgb
  # - domfountain_station1_xyz_intensity_rgb
  # steps_per_epoch_train: 500
  # steps_per_epoch_valid: 10
model:
  name: RandLANet
  batcher: DefaultBatcher
  # device: 'cpu'
  # ckpt_path: logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00100.pth # continue from this checkpoint (=saved weights)
  # is_resume: True
  num_neighbors: 16
  num_layers: 5
  num_points: 65536 # A random sample of num_points will be taken from each point cloud as input for the model
  num_classes: 2
  ignored_label_inds: []
  sub_sampling_ratio: [4, 4, 4, 4, 2] # 
  in_channels: 3
  dim_features: 3
  dim_output: [16, 64, 128, 256, 512]
  grid_size: 0.02 # Point clouds will be preprocessed with this grid size
  augment:
    recenter:
      dim: [0, 1]
    normalize:
      feat:
        method: linear
        bias: 0
        scale: 255
    rotate:
      method: vertical
    scale:
      min_s: 0.9
      max_s: 1.1
    noise:
      noise_std: 0.001
pipeline:
  name: SemanticSegmentation
  device: 'cuda:0'
  optimizer:
    lr: 0.001
  batch_size: 1
  main_log_dir: /project/logs
  max_epoch: 300
  save_ckpt_freq: 5
  scheduler_gamma: 0.9886
  test_batch_size: 1
  split: train
  train_sum_dir: /project/train_log
  val_batch_size: 2
  summary:
    record_for: []
    max_pts:
    use_reference: false
    max_outputs: 1